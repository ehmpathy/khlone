# khlone v0: zoomin — brainCli lookup

> how khlone finds, reuses, or spawns brainCli instances for clones

---

## the question

the vision says:

```
1. khlone looks up "grok" in khlone.yml → opencode@xai/grok/code-fast-1
2. khlone passes the brain slug to rhachet for resolution
3. rhachet resolves the slug → returns BrainCli instance
4. rhachet returns BrainCli instance
5. clone calls brainCli.act({ prompt, role, ... })
```

but this flow assumes every dispatch spawns a fresh BrainCli. in practice:
- clones persist across tasks — foreman.1 handles task-001, then task-002, then task-003
- their brainCli process stays alive between tasks (daemon holds the handle)
- khlone must decide: **reuse the extant brainCli or spawn a new one?**

this zoomin explores that decision via playtests.

---

## playtest.1 — first task to a fresh zone

**scenario:** user dispatches first task ever in `@feat/auth`

```sh
$ khlone act "implement auth"
```

**what happens:**

```
1. khlone detects no zone state for @feat/auth
2. khlone auto-inits zone
3. khlone looks up hero config → role: foreman, brain: claude
4. khlone looks up "claude" alias → claude@anthropic/claude/opus/v4.5
5. khlone checks: is there an extant brainCli for foreman.1? → no
6. khlone calls rhachet to spawn a new brainCli from slug
7. rhachet returns brainCli instance (pid: 12345)
8. khlone registers foreman.1 → { brainCli, pid: 12345, status: active }
9. khlone dispatches task-001 via brainCli.act("implement auth")
```

**key:** rhachet is only called because no extant brainCli exists for this clone.

---

## playtest.2 — second task to same clone

**scenario:** user queues another task while foreman.1 is active

```sh
$ khlone act "add tests"
```

**what happens:**

```
1. khlone looks up hero → foreman.1
2. khlone checks: is there an extant brainCli for foreman.1? → yes (pid: 12345)
3. khlone queues task-002 to foreman.1
4. when task-001 completes, foreman.1 picks up task-002
5. foreman.1 dispatches via the SAME brainCli.act("add tests")
```

**key:** no rhachet call. the brainCli is alive and reused. the process persists.

---

## playtest.3 — enroll a new clone of a different role

**scenario:** user wants a researcher while foreman.1 is busy

```sh
$ khlone ask "research auth patterns" --who researcher++
```

**what happens:**

```
1. khlone parses --who researcher++ → enroll new researcher clone
2. khlone looks up "researcher" role alias → ehmpathy/researcher
3. khlone determines brain for new clone → hero default: claude
4. khlone looks up "claude" alias → claude@anthropic/claude/opus/v4.5
5. khlone checks: is there an extant brainCli for researcher.1? → no (just enrolled)
6. khlone calls rhachet to spawn a new brainCli from slug
7. rhachet returns brainCli instance (pid: 12346)
8. khlone registers researcher.1 → { brainCli, pid: 12346, status: active }
9. khlone dispatches task-003 via brainCli.ask("research auth patterns")
```

**key:** new clone = new brainCli spawn via rhachet. each clone owns its own brainCli process.

---

## playtest.4 — task with @brain override via --who

**scenario:** user wants a task done with grok instead of the hero's default claude

```sh
$ khlone act "quick fix" --who @grok
```

**what happens:**

```
1. khlone parses --who @grok → { role: hero, brain: grok, index: null }
2. khlone looks up "grok" alias → opencode@xai/grok/code-fast-1
3. different brain = different clone — this is a separate brainCli process
4. khlone finds or enrolls hero clone on grok (e.g., foreman.{n})
5. khlone calls rhachet to spawn brainCli from grok slug
6. rhachet returns grokBrainCli (pid: 12347)
7. khlone registers foreman.{n} → { brainCli, pid: 12347, brain: grok }
8. khlone dispatches task-004 via grokBrainCli.act("quick fix")
9. foreman.1 (claude, pid: 12345) is unaffected — continues its own queue
```

**key:** different brain → different clone. always. a clone is bound to its brain — a brain swap mid-clone would break session continuity, checkpoint state, and the mental model. if you want a different brain, you get a different clone. the original clone's brainCli is never killed or replaced.

---

## playtest.5 — clone crash and recovery

**scenario:** foreman.1's brainCli process crashes mid-task

```
1. khlone detects brainCli process exit (pid: 12345 died)
2. khlone marks foreman.1 status: crashed
3. khlone reads last checkpoint for task-002
4. khlone reads foreman.1's BrainSeries ref from clone state
5. khlone calls rhachet to spawn a new brainCli from same slug
   — passes the BrainSeries ref so rhachet can continue the prior series
6. rhachet returns new brainCli (pid: 12349)
   — new brainCli resumes within the same BrainSeries (new episode, same series)
7. khlone registers foreman.1 → { brainCli, pid: 12349, status: active }
8. khlone resumes task-002 via brainCli.act({ resume from checkpoint })
```

**key:** crash recovery = spawn fresh brainCli via rhachet with the same BrainSeries. the clone identity persists, the series continuity persists — only the brainCli process is replaced. the new episode within the series means the clone retains cross-context memory from prior episodes.

---

## playtest.6 — shell exit and reattach

**scenario:** user closes terminal, reopens later

```sh
# terminal 1 (closes)
$ khlone act "implement auth"
✓ task-001 → foreman.1
$ exit

# terminal 2 (later)
$ khlone status

zone @feat/auth (local)
├─ ● foreman.1  67%  implement auth
└─ queue 0 tasks
```

**what happens:**

```
1. user exits shell → khlone CLI process exits
2. brainCli process (pid: 12345) is NOT a child of the shell — it's owned by the daemon
3. daemon keeps brainCli alive
4. user opens new shell, runs khlone status
5. khlone connects to daemon via unix socket
6. daemon reports foreman.1 status from its live brainCli
```

**key:** the daemon owns the brainCli processes, not the user's shell. this is what enables headless persistence.

---

## why no terminal multiplexer

khlone's daemon eliminates the need for tmux, screen, or any external terminal multiplexer. the daemon manages brainCli sessions directly — spawn, persist, attach, detach, monitor, restart — so users never need to `tmux new-session -d 'claude'` themselves.

| what tmux does | what khlone's daemon does |
|----------------|--------------------------|
| `tmux new-session -d` | daemon spawns brainCli via rhachet |
| session persists after shell exit | brainCli persists — daemon owns the handle |
| `tmux attach -t sess` | `khlone talk` attaches to clone's brainCli |
| `ctrl+b d` (detach) | `/exit` detaches, clone continues |
| `tmux send-keys "cmd"` | daemon writes via `brainCli.terminal.write()` |
| `tmux capture-pane` | `khlone watch` streams brainCli output |
| `tmux kill-session` | daemon kills brainCli on clone shutdown |

the daemon is a purpose-built, single-concern session manager for brainCli processes. it's not a general terminal multiplexer — it only manages brain CLI processes. but for that scope, it fully subsumes the multiplexer role.

PTY management is internal to the BrainCli supplier (e.g., `rhachet-brains-anthropic` uses node-pty under the hood). khlone never touches PTY directly — it operates through the BrainCli contract. this avoids external dependencies like tmux and the overhead of tmux output parse.

---

## daemon scope: one daemon per zone, with clone crash isolation

### the decision: per-zone daemon

```
zone (@feat/auth)
└── daemon (pid: 1000)
    ├── brainCli: foreman.1 (pid: 1001)    [separate brainCli handle]
    ├── brainCli: mechanic.1 (pid: 1002)   [separate brainCli handle]
    └── brainCli: researcher.1 (pid: 1003) [separate brainCli handle]
```

- one daemon per zone, one unix socket per zone: `/tmp/khlone-zone-{zone-id}.sock`
- daemon manages all clones in the zone
- daemon routes commands to the right clone's brainCli internally
- each clone has its own separate brainCli handle for crash isolation

### why per-zone, not per-clone

per-clone daemons multiply the `O(clones)` overhead on the daemon layer — which is pure waste, since the brain CLIs themselves are already `O(clones)`.

| resource | per-clone (5 clones) | per-zone (5 clones) |
|----------|---------------------|---------------------|
| daemon memory | ~150-250MB (5 node.js processes) | ~30-50MB (1 node.js process) |
| unix sockets | 5 | 1 |
| process table entries | 5 daemons + 5 brain CLIs | 1 daemon + 5 brain CLIs |
| libuv thread pools | ~20 threads | ~4 threads |

at scale (10 zones × 3 clones), per-clone daemons consume ~1.2GB in daemon overhead alone — before any brain CLI starts. per-zone keeps it at ~400MB.

the brain CLIs are the heavy resource consumers. no reason to double the process count with per-clone daemons.

### crash isolation invariant

**a clone crash must never take down the daemon or peer clones.**

the daemon enforces this via:
- each clone has its own separate brainCli handle (separate process)
- daemon detects crashes via `brainCli.terminal.onExit` per clone — a crash in foreman.1's brainCli is isolated to that handle
- daemon restarts just the crashed clone (see playtest.5 for recovery flow)
- peer clones are unaffected — their brainCli processes continue

this gives the crash isolation benefit of per-clone daemons without the resource overhead.

### daemon crash (the one real tradeoff)

if the daemon itself crashes, all clones in that zone lose their supervisor. mitigation:
- daemon is a thin supervisor — minimal logic, minimal crash surface
- orchestrator monitors daemon health and restarts it
- on daemon restart, it re-attaches to extant brainCli processes via pid (if still alive) or respawns them via BrainSeries continuation

---

## the refined lookup flow

based on these playtests, the actual flow has a **find-or-enroll** pattern:

```
user: khlone act "task" --who researcher@grok++

1. khlone parses --who → { role: researcher, brain: grok, index: ++ }
2. khlone resolves role alias ("researcher" → ehmpathy/researcher via khlone.yml)
3. khlone resolves brain alias ("grok" → brain slug via khlone.yml)
4. khlone resolves target clone (role + brain + index)
   — clone identity = role + brain + index
   — different role or different brain = different clone
5. if index is ++:
   → force-enroll: spawn new clone via rhachet, assign next index
6. if index is null:
   → find extant clone with role + brain match
   → if found: reuse extant brainCli
   → if not found: auto-enroll new clone via rhachet, assign next index
7. if index is .n:
   → find clone by exact slug (role.n)
   → if @brain specified: assert brain matches
   → if not found: error
8. khlone dispatches task via brainCli.ask() or brainCli.act()
```

### the contract boundary

```
khlone owns:
├── clone lifecycle (enroll, idle, active, crashed)
├── brainCli process registry (which clones have live processes)
├── daemon process management (spawn, monitor, restart)
├── task queue (enqueue, dequeue, prioritize)
├── brain alias resolution (khlone.yml alias → brain slug)
└── find-or-enroll decision (reuse extant vs spawn fresh)

rhachet owns:
├── brain slug resolution (slug → BrainCli instance)
├── BrainCli contract (spawn, send, attach, detach, kill)
├── supplier package registry (which packages implement BrainCli)
└── BrainOutput metrics (tokens, cost, duration, continuation refs)
```

khlone never calls `rhachet.spawnBrainCli()` if an extant one already serves the clone with the right brain. rhachet is the factory; khlone is the fleet manager.

---

## deferred to post-v0

1. **idle timeout** — khlone.yml attribute for brainCli inactivity timeout. not v0 scope.
2. **max clones** — khlone.yml attribute for concurrent brainCli limit (per zone, per site, per machine). not v0 scope.
