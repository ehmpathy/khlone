# block C: clone crash recovery (in-daemon, stateless)

> when a clone's brain CLI process dies, the daemon catches it, boots a fresh process via the handle, and resumes the active task. no persistence needed — the daemon holds all state in memory. this is clone-level crash recovery only; daemon-level crash recovery requires block F.

---

## read first

1. `./1.vision.md` — sections: "daemon architecture" (crash isolation invariant, separate BrainCli handles), "brain supplier architecture" (BrainSeries for cross-context continuation)
2. `./2.1.criteria.blackbox.md` — usecase 12 (crash recovery)
3. `./3.3.blueprint.v1.i1.md` — codepath 2.7 (crash recovery: brainCli.terminal.onExit → executor.boot → re-dispatch)
4. `./3.4.blueprint.handoff.contract.braincli.v1.i1.md` — BrainCli contract: `executor.boot()`, `terminal.onExit`, series preservation
5. `./3.5.decomposition.v1.i1.md` — block C section: "why no persistence", boundaries

---

## prerequisite

**block A must be complete.** the daemon must exist with a live brainCli handle for the hero clone.

---

## scope

### what to build

**daemon enhancements (2 prod, 2 test):**
- `genCloneViaDaemon` — ensure clone process alive; if dead, respawn via `setCloneViaDaemon`
- `setCloneViaDaemon` — respawn clone's brainCli via BrainSeries continuation: read series ref from handle → call `brainCli.executor.boot({ mode: 'dispatch' })` → re-dispatch the active task
- both wire `brainCli.terminal.onExit` callback per clone handle — detect when a clone's brain CLI process exits unexpectedly

**acceptance (1 test):**
- khlone.crash — clone crashes mid-task → daemon restarts it → task resumes → peer clones (once block D lands) unaffected

### why no persistence

the daemon is still alive when a clone crashes. it holds:
- the clone's `BrainSeries` ref on the handle (used by `executor.boot` to continue the same series)
- the active task state in memory (which task was active, what prompt)
- the task queue in memory

`brainCli.executor.boot()` uses the series ref from the handle. no disk read needed. the restarted clone resumes in a new episode within the same series.

**daemon-level crash** (the supervisor itself dies) is a separate concern — it requires persistence and is handled in block F. this block handles **clone-level crash only**.

### what NOT to build

- **no checkpoint persistence.** no `daoCloneCheckpoint`. the daemon holds the resume state in memory.
- **no transcript.** clone crash events are not logged to disk.
- **no daemon-level crash recovery.** if the daemon dies, all state is lost until block F.

---

## file count

| layer | prod | test | total |
|-------|------|------|-------|
| daemon enhancements (onExit handler, boot path) | 2 | 2 | 4 |
| acceptance (crash) | 0 | 1 | 1 |
| **total** | **2** | **3** | **5** |

---

## usecases fulfilled

| usecase | status |
|---------|--------|
| usecase.12: crash recovery (clone-level) | **yes** — crash → restart → resume via BrainSeries |

---

## playtests

```sh
# clone crashes mid-task
# daemon detects via brainCli.terminal.onExit callback
# daemon boots brainCli via handle (series preserved)
# task resumes — new episode, same series

$ khlone status
zone @feat/auth (local)
├─ ● foreman.1  restarted  implement auth (resumed)
└─ queue 0 tasks
```

---

## done when

- [ ] daemon catches clone process exit via `brainCli.terminal.onExit` callback
- [ ] daemon calls `brainCli.executor.boot({ mode: 'dispatch' })` on crash
- [ ] booted clone continues in the same BrainSeries (new episode)
- [ ] active task resumes after boot
- [ ] peer clones (when present via block D) are unaffected by one clone's crash — each clone has its own separate BrainCli handle
- [ ] acceptance test: force-kill clone process → verify daemon restarts it → verify task resumes
